{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d43baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 00:05:04.857265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-15 00:05:06.219289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Activation, Conv1D, MaxPooling1D, Dropout, Lambda, LeakyReLU\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b1ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('/home/mahinur/Desktop/CSV_1.csv')\n",
    "data2 = pd.read_csv('/home/mahinur/Desktop/CSV_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32c6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(data1, data2, on='sid')\n",
    "numeric_columns = merged_data.select_dtypes(include=[float, int]).columns\n",
    "merged_data = merged_data[numeric_columns]\n",
    "\n",
    "# Normalize the merged data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = pd.DataFrame(scaler.fit_transform(merged_data), columns=merged_data.columns)\n",
    "\n",
    "# Save the normalized data to a new CSV file\n",
    "normalized_data.to_csv('/home/mahinur/Desktop/normalized_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b8b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: OrderedDict([('gamma', 0.10325309897613151), ('learning_rate', 0.08905744151836509), ('max_depth', 3), ('min_child_weight', 1), ('n_estimators', 188)])\n",
      "Test Accuracy: 0.9171\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      1372\n",
      "         1.0       0.43      0.02      0.05       123\n",
      "\n",
      "    accuracy                           0.92      1495\n",
      "   macro avg       0.67      0.51      0.50      1495\n",
      "weighted avg       0.88      0.92      0.88      1495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the normalized data from the CSV file\n",
    "normalized_data = pd.read_csv('/home/mahinur/Desktop/normalized_data.csv')\n",
    "\n",
    "# Extract the features (X) and target (y) columns\n",
    "X = normalized_data.drop('output1', axis=1).values\n",
    "y = normalized_data['output1'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter search space with adjusted bounds\n",
    "param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'max_depth': (3, 11),\n",
    "    'n_estimators': (50, 201),\n",
    "    'gamma': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (1, 11),\n",
    "}\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Perform Bayesian optimization for hyperparameter search\n",
    "opt = BayesSearchCV(model, param_space, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = opt.best_estimator_\n",
    "best_params = opt.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Predict the target values using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9e4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d4032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
